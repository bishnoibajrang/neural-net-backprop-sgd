# Parameter Optimization via Backpropagation using PyTorch

This project demonstrates the optimization of neural network parameters using backpropagation and stochastic gradient descent (SGD) in PyTorch.

## ğŸ“Œ Highlights
- Built a simple neural network using `torch.nn.Module`
- Implemented forward and backward propagation using SGD
- Visualized training loss using `matplotlib` and `seaborn`
- Explained learning rates, activation functions, and convergence

## ğŸ“ Files
- `backprop_pytorch_sgd.ipynb` â€” Main notebook

## ğŸ§  Dependencies
- Python 3.8+
- PyTorch
- matplotlib
- seaborn

## ğŸš€ How to Run
```bash
jupyter notebook backprop_pytorch_sgd.ipynb
```
