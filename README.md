# Parameter Optimization via Backpropagation using PyTorch

This project demonstrates the optimization of neural network parameters using backpropagation and stochastic gradient descent (SGD) in PyTorch.

## 📌 Highlights
- Built a simple neural network using `torch.nn.Module`
- Implemented forward and backward propagation using SGD
- Visualized training loss using `matplotlib` and `seaborn`
- Explained learning rates, activation functions, and convergence

## 📁 Files
- `backprop_pytorch_sgd.ipynb` — Main notebook

## 🧠 Dependencies
- Python 3.8+
- PyTorch
- matplotlib
- seaborn

## 🚀 How to Run
```bash
jupyter notebook backprop_pytorch_sgd.ipynb
```
